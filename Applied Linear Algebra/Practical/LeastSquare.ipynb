{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Estimation\n",
    "\n",
    "## Instructions\n",
    "In this assignment you will use LS estimation in two datasets and compare it to Gradient Descent.\n",
    "\n",
    "#### 1. hight and weight:\n",
    "given hights of people you will estimate weight of those people by LS and then compare it with Gradient Descent.\n",
    "\n",
    "#### 2. Load News popularity data:\n",
    "This is a dataset of news with some parameters to predict how many shares every news get.\n",
    "To get more familar with the dataset please open `OnlineNewsPopularity.csv` and check the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Least Squares Estimation\n",
    "Least squares estimation is one of the fundamental machine learning algorithms.\n",
    "Consider the case where we have a linear model for predicting housing prices. We are predicting the housing prices based on features in the \n",
    "housing dataset. If we denote the features as $\\boldsymbol x_0, \\dotsc, \\boldsymbol x_n$ and collect them into a vector $\\boldsymbol {x}$, and the price of the houses as $y$. Assuming that we have \n",
    "a prediction model in the way such that $\\hat{y}_i =  f(\\boldsymbol {x}_i) = \\boldsymbol \\theta^T\\boldsymbol {x}_i$.\n",
    "\n",
    "\n",
    "If we collect the dataset into a $(N,D)$ data matrix $\\boldsymbol X$, we can write down our model like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\boldsymbol{x}_1^T \\\\\n",
    "\\vdots \\\\ \n",
    "\\boldsymbol{x}_N^T \n",
    "\\end{bmatrix} \\boldsymbol{\\theta} = \\begin{bmatrix} \n",
    "y_1 \\\\\n",
    "\\vdots \\\\ \n",
    "y_2 \n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "i.e.,\n",
    "\n",
    "$$\n",
    "\\boldsymbol X\\boldsymbol{\\theta} = \\boldsymbol{y}.\n",
    "$$\n",
    "\n",
    "Note that the data points are the *rows* of the data matrix, i.e., every column is a dimension of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the first data: Height and weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_first_data(sub_sample=True, add_outlier=False):\n",
    "    \"\"\"Load data and convert it to the metric system.\"\"\"\n",
    "    \n",
    "    path_dataset = \"height_weight_genders.csv\"\n",
    "    \n",
    "    data = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", skip_header=1, usecols=[1, 2])\n",
    "    height = data[:, 0]\n",
    "    weight = data[:, 1]\n",
    "    gender = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", skip_header=1, usecols=[0],\n",
    "        converters={0: lambda x: 0 if b\"Male\" in x else 1})\n",
    "    # Convert to metric system\n",
    "    height *= 0.025\n",
    "    weight *= 0.454\n",
    "\n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        height = height[::50]\n",
    "        weight = weight[::50]\n",
    "\n",
    "    if add_outlier:\n",
    "        # outlier experiment\n",
    "        height = np.concatenate([height, [1.1, 1.2]])\n",
    "        weight = np.concatenate([weight, [51.5/0.454, 55.3/0.454]])\n",
    "    \n",
    "    return height, weight, gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the data\n",
    "In this function we will standarize the dataset:\n",
    "$$x = \\frac{x - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"\n",
    "    Standardize the original data set.\n",
    "    :param x: the numpy array \n",
    "    :returns: standardized_x, mean_x, std_x\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "    raise NotImplementedError # delete this line after finishing this function\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_data(height, weight):\n",
    "    \"\"\"Form (y,tX) to get regression data in matrix form.\"\"\"\n",
    "    y = weight\n",
    "    x = height\n",
    "    num_samples = len(y)\n",
    "    tx = np.c_[np.ones(num_samples), x]\n",
    "    return y, tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Height and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, weight, gender = load_first_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "b, A = build_model_data(x, weight)\n",
    "\n",
    "print(f\"mean_x, std_x: {mean_x, std_x}\")\n",
    "b.shape, A.shape\n",
    "epsilon = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(nrows, ncols, plot_number)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(height, b)\n",
    "plt.xlabel('unnormalized height')\n",
    "plt.ylabel('weight')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(A[:, 1], b)\n",
    "plt.xlabel('normalized height')\n",
    "plt.ylabel('weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate LS\n",
    "Consider the case where we have a linear model for predicting housing prices. We are predicting the housing prices based on features in the \n",
    "housing dataset. If we denote the features as $\\boldsymbol x_0, \\dotsc, \\boldsymbol x_n$ and collect them into a vector $\\boldsymbol {x}$, and the price of the houses as $y$. Assuming that we have \n",
    "a prediction model in the way such that $\\hat{y}_i =  f(\\boldsymbol {x}_i) = \\boldsymbol \\theta^T\\boldsymbol {x}_i$.\n",
    "\n",
    "\n",
    "If we collect the dataset into a $(N,D)$ data matrix $\\boldsymbol X$, we can write down our model like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\boldsymbol{x}_1^T \\\\\n",
    "\\vdots \\\\ \n",
    "\\boldsymbol{x}_N^T \n",
    "\\end{bmatrix} \\boldsymbol{\\theta} = \\begin{bmatrix} \n",
    "y_1 \\\\\n",
    "\\vdots \\\\ \n",
    "y_2 \n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "i.e.,\n",
    "\n",
    "$$\n",
    "\\boldsymbol X\\boldsymbol{\\theta} = \\boldsymbol{y}.\n",
    "$$\n",
    "\n",
    "Note that the data points are the *rows* of the data matrix, i.e., every column is a dimension of the data. \n",
    "\n",
    "Our goal is to find the best $\\boldsymbol\\theta$ such that we minimize the following objective (least square).\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "& \\sum^n_{i=1}{\\lVert \\bar{y_i} - y_i \\rVert^2} \\\\\n",
    "&= \\sum^n_{i=1}{\\lVert \\boldsymbol \\theta^T\\boldsymbol{x}_i - y_i \\rVert^2} \\\\\n",
    "&= (\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y)^T(\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "If we set the gradient of the above objective to $\\boldsymbol  0$, we have\n",
    "$$\n",
    "\\begin{eqnarray} \n",
    "\\nabla_\\theta(\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y)^T(\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y) &=& \\boldsymbol 0 \\\\\n",
    "\\nabla_\\theta(\\boldsymbol {\\theta}^T\\boldsymbol X^T - \\boldsymbol y^T)(\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y) &=& \\boldsymbol 0 \\\\\n",
    "\\nabla_\\theta(\\boldsymbol {\\theta}^T\\boldsymbol X^T\\boldsymbol X\\boldsymbol {\\theta} - \\boldsymbol y^T\\boldsymbol X\\boldsymbol \\theta - \\boldsymbol \\theta^T\\boldsymbol X^T\\boldsymbol y + \\boldsymbol y^T\\boldsymbol y ) &=& \\boldsymbol 0 \\\\\n",
    "2\\boldsymbol X^T\\boldsymbol X\\theta - 2\\boldsymbol X^T\\boldsymbol y &=& \\boldsymbol 0 \\\\\n",
    "\\boldsymbol X^T\\boldsymbol X\\boldsymbol \\theta        &=& \\boldsymbol X^T\\boldsymbol y.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The solution that gives zero gradient solves (which we call the maximum likelihood estimator) the following equation:\n",
    "\n",
    "$$\\boldsymbol X^T\\boldsymbol X\\boldsymbol \\theta = \\boldsymbol X^T\\boldsymbol y.$$\n",
    "\n",
    "_This is exactly the same as the normal equation we have for projections_.\n",
    "\n",
    "This means that if we solve for $\\boldsymbol X^T\\boldsymbol X\\boldsymbol \\theta = \\boldsymbol X^T\\boldsymbol y.$ we would find the best $\\boldsymbol \\theta = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y$, i.e. the $\\boldsymbol \\theta$ which minimizes our objective.\n",
    "\n",
    "In this section we try to calculate the LS for prdicting our data by solving the exact equation:\n",
    "\n",
    "$$A^t A x = A^t b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `solve_least_square` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_least_square(A, b):\n",
    "    # YOUR CODE HERE\n",
    "    # You can use library routines in `np.linalg.*`\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the line\n",
    "Call `solve_least_square` and plot the line on the data.\n",
    "\n",
    "you can use `plt.plot(x, ax+b, \"RED\")` for plotting the y=ax+b line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradient Descent\n",
    "Gradient Descent is an iterative optimization algorithm to find the minimum of a function.\n",
    "\n",
    "Here that function is our Loss Function which is the Least Square function.\n",
    "\n",
    "In this section, we try to calculate the LS of the same data by an iterative algorithm.\n",
    "we have LS as our loss function. Our goal is to minimize this loss function to obtain the most accurate value for weight.\n",
    "<img src=\"gd.png \" width=\"600\" align=\"center\">\n",
    "This algorithm consists of 4 steps:\n",
    "### Step 1\n",
    "initialling the values to x (ex. [1, 1]).\n",
    "\n",
    "Let (`L`) be our learning rate. This controls how much the value of x changes with each step. L could be a small value for good accuracy.\n",
    "\n",
    "### Step 2\n",
    "Calculate the gradient of the loss function (`G`).\n",
    "\n",
    "### Step 3\n",
    "Now we update the current value of `x` using the following equation:\n",
    "`x` = `x` - `L` * `G`\n",
    "\n",
    "### Step 4\n",
    "We repeat this process until our loss function is a very small value or ideally 0 (which means 0 error or 100% accuracy).\n",
    "\n",
    "The value of x that we are left with now will be the optimum value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Objective Function\n",
    "Fill in the `calculate_objective` function below:\n",
    "\n",
    "where `Axmb` is given and we just want to compute the LS where  `Axmb` = $Ax - b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_objective(Axmb):\n",
    "    \"\"\"\n",
    "    Calculate the mean squared error for vector Axmb = Ax - b.\n",
    "    :param Axmb: the n*1 vector of Ax - b\n",
    "    :return: copmutes the least square of Ax-b\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `compute_gradient` below:\n",
    "\n",
    "you should calculate the gradient of LS function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(b, A, x):\n",
    "    \"\"\"\n",
    "    Computing the gradient of least square error\n",
    "    :param b: the output that we should estimate\n",
    "    :param A: the input data that we use it to get b\n",
    "    :param x: the vector of weights that should minimize the loss functions\n",
    "    :return: returns the gradient of LS in x, and also Ax-b (as Axmb)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    return grad, Axmb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:\n",
    "\n",
    "where `gamma` is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(b, A, initial_x, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store x and objective func. values\n",
    "    xs = [initial_x]\n",
    "    objectives = [-1]\n",
    "    x = initial_x\n",
    "    for n_iter in range(max_iters):\n",
    "        # TODO: compute gradient and objective function\n",
    "        \n",
    "\n",
    "        # TODO: update x by a gradient descent step\n",
    "        \n",
    "        # TODO: store x and objective function value\n",
    "\n",
    "        \n",
    "        \n",
    "        if abs(obj - objectives[-2]) < epsilon:\n",
    "                break\n",
    "        print(\"Gradient Descent({bi}/{ti}): objective={l}\".format(bi=n_iter, ti=max_iters - 1, l=obj))\n",
    "    return objectives, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your data\n",
    "\n",
    "You shoud assing values to parameters `max_iters` and `gamma`. you can use `Plotting the lines‍‍‍` and `Plotting the cost per iterations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Define the parameters of the algorithm. you can use `Plotting the lines‍‍‍` part to check the parameters you choose.\n",
    "\n",
    "max_iters = # maximum iteration of the algorithm\n",
    "\n",
    "gamma = # how big or small every steps are (usually between 0.000001 to 0.1 is ok!)\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_objectives_naive, gradient_xs_naive = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(A[:, 1], b)\n",
    "for i in range(1, len(gradient_xs_naive)):\n",
    "    beta, alpha = gradient_xs_naive[i]\n",
    "    plt.plot(x, alpha *x + beta, \"RED\")\n",
    "    plt.title(f'objective={gradient_objectives_naive[-1]}, y = {alpha}*x + {beta}', color=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the cost per iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gradient_objectives_naive[1:])\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost per iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load News popularity data\n",
    "This is a dataset of news with parameters for prediction how many shares every news get.\n",
    "To get more familar with the dataset please open `OnlineNewsPopularity.csv` and check the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_news_data(sub_sample=True):\n",
    "    \"\"\"Load data and convert it to the metric system.\"\"\"\n",
    "    \n",
    "    path_dataset = \"OnlineNewsPopularity.csv\"\n",
    "    \n",
    "    data = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", skip_header=1)\n",
    "    \n",
    "    data = data[:, 1:-2]\n",
    "    shares = data[:, -1]\n",
    "\n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        data = data[::50]\n",
    "        shares = shares[::50]\n",
    "    \n",
    "    return data, shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, shares = load_news_data(sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape,shares.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute LS using exact formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_least_square(A, b):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the estimation\n",
    "you can compare your estimation to the real value of `shares` bellow:\n",
    "you can also plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (OPTIONAL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
